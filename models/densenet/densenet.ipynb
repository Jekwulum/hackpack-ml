{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "In this notebook, we train a DenseNet classifier for SVHN and CIFAR10 datasets, and launch it in a web API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script adapted from: https://github.com/kuangliu/pytorch-cifar\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from densenet import densenet121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "Here, we load either the SVHN or CIFAR10 datasets, which are provided through torchvision. If you wish to use your own, ...\n",
    "\n",
    "Note that we are treating the test set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from PIL image format to tensor format\n",
    "transform_train = transforms.Compose([\n",
    "    # You can add more data augmentation techniques in series: \n",
    "    # https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# CIFAR10 Dataset: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# SVHN Dataset: http://ufldl.stanford.edu/housenumbers/\n",
    "# trainset = torchvision.datasets.SVHN(root='../data', split='train', transform=transform_train, download=True)\n",
    "# testset = torchvision.datasets.SVHN(root='../data', split='test', transform=transform_test, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If making a proof of concept application, we can choose to overfit on a data subset for quick training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ct = 128 # Size of train data\n",
    "test_ct = 32 # Size of test data\n",
    "batch_sz = 32 # Size of batch for one gradient step (bigger batches take more memory but are faster)\n",
    "num_workers = 4 # 4 * number of GPUs\n",
    "\n",
    "if train_ct:\n",
    "    trainset = data.dataset.Subset(trainset, range(train_ct))\n",
    "\n",
    "if test_ct:\n",
    "    testset = data.dataset.Subset(testset, range(test_ct))\n",
    "\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_sz, shuffle=True, num_workers=num_workers, )\n",
    "testloader = data.DataLoader(testset, batch_size=batch_sz, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First, we configure the model for training, and define our loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpu_ids = [0] # On Colab, we have access to one GPU. Change this value as you see fit\n",
    "\n",
    "net = densenet121()\n",
    "net = net.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net, gpu_ids)\n",
    "    \n",
    "resume = False # Resume training from a saved checkpoint\n",
    "\n",
    "if resume:\n",
    "    print('Resuming from checkpoint at ./checkpoint/best_model.pth.tar')\n",
    "    assert os.path.isdir('ckpts'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/best_model.pth.tar')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    global best_loss\n",
    "    best_loss = checkpoint['test_loss']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "# Loss function: https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `train`, which performs a forward/back propagation pass on our dataset per epoch. Similarly, `test` performs evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Trains our net on data from the trainloader for a single epoch\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=len(trainloader.dataset)) as progress_bar:\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # Clear any stored gradients for new step\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            loss = loss_fn(outputs, targets) # Calculate loss between prediction and label     \n",
    "            loss.backward() # Backpropagate gradient updates through net based on loss\n",
    "            optimizer.step() # Update net weights based on gradients\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            acc = (100. * correct / total)\n",
    "        \n",
    "            progress_bar.set_postfix(loss=train_loss/(batch_idx+1), accuracy=f'{acc}%')\n",
    "            progress_bar.update(inputs.size(0))\n",
    "            \n",
    "        \n",
    "def test(epoch):\n",
    "    \"\"\"\n",
    "    Run net in inference mode on test data. \n",
    "    \"\"\"                       \n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_acc = 0\n",
    "    # Ensures the net will not update weights\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(testloader.dataset)) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "            \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                acc = (100. * correct / total)\n",
    "                progress_bar.set_postfix(loss=test_loss/(batch_idx+1), accuracy=f'{acc}%')\n",
    "                progress_bar.update(inputs.size(0))\n",
    "                \n",
    "                # Save best model\n",
    "                if acc > best_acc:\n",
    "                    print(\"Saving...\")\n",
    "                    save_state(net, acc, epoch)\n",
    "                    best_acc = acc\n",
    "\n",
    "def save_state(net, acc, epoch):\n",
    "    \"\"\"\n",
    "    Save the current net state, accuracy and epoch\n",
    "    \"\"\"\n",
    "    state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_freq = 5 # Frequency to run model on validation data\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    train(epoch)\n",
    "    if epoch % test_freq == 0:\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that we have trained a model, we can use it for inference on new data! With each run, we save the best model weights at:\n",
    "`./checkpoint/best_model.pth.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(x):\n",
    "    outputs = net(x)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = torch.load('./checkpoint/best_model.pth.tar')\n",
    "net.load_state_dict(cp['net'])\n",
    "net.eval()\n",
    "\n",
    "sample = trainset[1][0]\n",
    "plt.imshow(sample.permute(1,2,0))\n",
    "\n",
    "y = classify_image(sample.unsqueeze(0))[0]\n",
    "print(f'Predicted class: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you are running on CIFAR, you can get the associated class labels at https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "0 = airplane, 1 = automobile, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export (API)\n",
    "We want to use our models inside our apps. One way to do this is to wrap our calls in an API. We included a `app.py` that takes the model saved at `models/densenet/checkpoint/best_model.pth.tar` and wraps it in a simple Flask server that receives images and returns the classification. Check out our web API hackpack for more information on how this works! (https://github.com/TreeHacks/hackpack-web-api)\n",
    "\n",
    "Congratulations! You have just trained and launched your own ML model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
