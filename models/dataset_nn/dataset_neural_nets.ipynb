{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Neural Networks\n",
    "This notebook will step through the process of loading an arbitrary dataset in PyTorch, and creating a simple neural network for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "We will first work through loading an arbitrary dataset in PyTorch. For this project, we chose the <a href=\"http://www.cs.toronto.edu/~delve/data/abalone/desc.html\">delve abalone dataset</a>. \n",
    "\n",
    "First, download and unzip the dataset from the link above, then unzip `Dataset.data.gz` and move `Dataset.data` into `hackpack-ml/models/data`.\n",
    "We are given the following attribute information in the spec:\n",
    "```\n",
    "Attributes:\n",
    "  1   sex                 u  M F I\t# Gender or Infant (I)\n",
    "  2   length              u  (0,Inf]\t# Longest shell measurement (mm)\n",
    "  3   diameter            u  (0,Inf]\t# perpendicular to length     (mm)\n",
    "  4   height              u  (0,Inf]\t# with meat in shell (mm)\n",
    "  5   whole_weight        u  (0,Inf]\t# whole abalone  (gr)\n",
    "  6   shucked_weight      u  (0,Inf]\t# weight of meat (gr)    \n",
    "  7   viscera_weight      u  (0,Inf]\t# gut weight (after bleeding) (gr)\n",
    "  8   shell_weight        u  (0,Inf]\t# after being dried (gr)\n",
    "  9   rings               u  0..29\t# +1.5 gives the age in years\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a data manipulation library that works really well with structured data. We can use Pandas DataFrames to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['sex', 'length', 'diameter', 'height', 'whole_weight', \n",
    "             'shucked_weight', 'viscera_weight', 'shell_weight', 'rings']\n",
    "abalone_df = pd.read_csv('../data/Dataset.data', sep=' ', names=col_names)\n",
    "abalone_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a subclass of PyTorch Dataset for our Abalone dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbaloneDataset(data.Dataset):\n",
    "    \"\"\"Abalone dataset. Provides quick iteration over rows of data.\"\"\"\n",
    "\n",
    "    def __init__(self, csv):\n",
    "        \"\"\"\n",
    "        Args: csv (string): Path to the Abalone dataset.\n",
    "        \"\"\"\n",
    "        self.features = ['sex', 'length', 'diameter', 'height', 'whole_weight', \n",
    "                          'shucked_weight', 'viscera_weight', 'shell_weight']\n",
    "        self.y = ['rings']\n",
    "        self.abalone_df = pd.read_csv(csv, sep=' ', names=(self.features + self.y))\n",
    "        \n",
    "        # Turn categorical data into machine interpretable format (one hot)\n",
    "        self.abalone_df['sex'] = pd.get_dummies(self.abalone_df['sex'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.abalone_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (x,y) pair where x are abalone features and y is age.\"\"\"\n",
    "        features = self.abalone_df.iloc[idx][self.features].values\n",
    "        y = self.abalone_df.iloc[idx][self.y]\n",
    "        return torch.Tensor(features).float(), torch.Tensor(y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "The task is to predict the age (number of rings) of abalone from physical measurements. We build a simple neural network with one hidden layer to model the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size):\n",
    "        super(Net, self).__init__()\n",
    "        # feature_size input channels (8), 1 output channels\n",
    "        self.fc1 = nn.Linear(feature_size, 4)\n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate an Abalone dataset instance and create DataLoaders for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AbaloneDataset('../data/Dataset.data')\n",
    "train_split, test_split = math.floor(len(dataset) * 0.8), math.ceil(len(dataset) * 0.2)\n",
    "\n",
    "trainset = [dataset[i] for i in range(train_split)]\n",
    "testset = [dataset[train_split + j] for j in range(test_split)]\n",
    "batch_sz = len(trainset) # Compact data allows for big batch size\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_sz, shuffle=True, num_workers=4)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_sz, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can initialize our network and define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(len(dataset.features))\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpu_ids = [0] # On Colab, we have access to one GPU. Change this value as you see fit\n",
    "\n",
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Trains our net on data from the trainloader for a single epoch\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    with tqdm(total=len(trainloader.dataset)) as progress_bar:\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad() # Clear any stored gradients for new step\n",
    "            outputs = net(inputs.float())\n",
    "            loss = loss_fn(outputs, targets) # Calculate loss between prediction and label  \n",
    "            loss.backward() # Backpropagate gradient updates through net based on loss\n",
    "            optimizer.step() # Update net weights based on gradients\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            progress_bar.update(inputs.size(0))\n",
    "            \n",
    "        \n",
    "def test(epoch):\n",
    "    \"\"\"\n",
    "    Run net in inference mode on test data. \n",
    "    \"\"\"                       \n",
    "    net.eval()\n",
    "    # Ensures the net will not update weights\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(testloader.dataset)) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device).float(), targets.to(device).float()\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                progress_bar.set_postfix(testloss=loss.item())\n",
    "                progress_bar.update(inputs.size(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is prepared, it's time to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_freq = 5 # Frequency to run model on validation data\n",
    "\n",
    "for epoch in range(0, 200):\n",
    "    train(epoch)\n",
    "    if epoch % test_freq == 0:\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the network's eval mode to do a sample prediction to see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "sample = testset[0]\n",
    "predicted_age = net(sample[0])\n",
    "true_age = sample[1]\n",
    "\n",
    "print(f'Input features: {sample[0]}')\n",
    "print(f'Predicted age: {predicted_age.item()}, True age: {true_age[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to load your own datasets into PyTorch and run models on it. For an example of Computer Vision, check out the DenseNet notebook. Happy hacking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
