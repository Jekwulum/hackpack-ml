{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "In this notebook, we train a DenseNet classifier for MNIST digits. (https://arxiv.org/abs/1608.06993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script adapted from: https://github.com/kuangliu/pytorch-cifar\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../..')\n",
    "from models import densenet121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "Here, we load the MNIST dataset, which is provided through torchvision. If you wish to use your own, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# Transform from PIL image format to tensor format\n",
    "transform_train = transforms.Compose([\n",
    "    # You can add more data augmentation techniques in series:\n",
    "    # https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# CIFAR10 Dataset: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "# trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "# testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# SVHN Dataset: http://ufldl.stanford.edu/housenumbers/\n",
    "trainset = torchvision.datasets.SVHN(root='../data', split='train', transform=transform_train, download=True)\n",
    "testset = torchvision.datasets.SVHN(root='../data', split='test', transform=transform_test, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If making a proof of concept application, we can choose to overfit on a data subset for quick training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ct = 10 # Size of train data\n",
    "test_ct = 20 # Size of test data\n",
    "batch_sz = 10\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "\n",
    "if train_ct:\n",
    "    trainset = data.dataset.Subset(trainset, range(train_ct))\n",
    "\n",
    "if test_ct:\n",
    "    testset = data.dataset.Subset(testset, range(test_ct))\n",
    "\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_sz, shuffle=True, num_workers=num_workers, )\n",
    "testloader = data.DataLoader(testset, batch_size=batch_sz, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = densenet121()\n",
    "net = net.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net, [0])\n",
    "    # cudnn.benchmark\n",
    "    \n",
    "resume = False # To resume training from saved checkpoint, \n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('Resuming from checkpoint at ckpts/best.pth.tar...')\n",
    "    assert os.path.isdir('ckpts'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('ckpts/best.pth.tar')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    global best_loss\n",
    "    best_loss = checkpoint['test_loss']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=len(trainloader.dataset)) as progress_bar:\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            acc = f'{(100. * correct / total)}%'\n",
    "        \n",
    "            progress_bar.set_postfix(loss=train_loss/(batch_idx+1), accuracy=acc)\n",
    "            progress_bar.update(inputs.size(0))\n",
    "            \n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(testloader.dataset)) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "            \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                acc = f'{(100. * correct / total)}%'\n",
    "                progress_bar.set_postfix(loss=test_loss/(batch_idx+1), accuracy=acc)\n",
    "                progress_bar.update(inputs.size(0))\n",
    "\n",
    "    best_acc = 0\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.t7')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.79it/s, accuracy=0.0%, loss=6.91]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.85it/s, accuracy=30.0%, loss=6.5]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.50it/s, accuracy=20.0%, loss=9.11]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.49it/s, accuracy=30.0%, loss=5.09]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.71it/s, accuracy=30.0%, loss=5.33]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.07it/s, accuracy=40.0%, loss=3.74]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.79it/s, accuracy=40.0%, loss=2.49]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.88it/s, accuracy=40.0%, loss=2.31]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.90it/s, accuracy=60.0%, loss=1.42]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.99it/s, accuracy=60.0%, loss=1.04]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.86it/s, accuracy=50.0%, loss=0.918]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.81it/s, accuracy=60.0%, loss=0.95]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.83it/s, accuracy=60.0%, loss=0.861]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.81it/s, accuracy=80.0%, loss=0.547]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.07it/s, accuracy=70.0%, loss=0.585]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.78it/s, accuracy=70.0%, loss=0.548]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.09it/s, accuracy=80.0%, loss=0.409]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.77it/s, accuracy=70.0%, loss=0.413]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.99it/s, accuracy=80.0%, loss=0.371]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.52it/s, accuracy=90.0%, loss=0.319]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.73it/s, accuracy=80.0%, loss=0.279]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.73it/s, accuracy=90.0%, loss=0.236]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.68it/s, accuracy=100.0%, loss=0.191]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.59it/s, accuracy=90.0%, loss=0.165]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.68it/s, accuracy=100.0%, loss=0.135]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.54it/s, accuracy=100.0%, loss=0.121]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.58it/s, accuracy=100.0%, loss=0.0986]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.67it/s, accuracy=100.0%, loss=0.0826]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.07it/s, accuracy=100.0%, loss=0.064]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.94it/s, accuracy=100.0%, loss=0.0424]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.98it/s, accuracy=100.0%, loss=0.0324]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.16it/s, accuracy=100.0%, loss=0.0173]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.05it/s, accuracy=100.0%, loss=0.0118]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b5363beb67ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     test(epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ae78bd77b723>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 100):\n",
    "    train(epoch)\n",
    "#     test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export (API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export (CoreML)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
